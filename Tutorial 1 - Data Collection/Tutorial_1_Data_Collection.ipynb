{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuFHM7XyUKxI"
   },
   "source": [
    "# Tutorial 1 - Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zksMzTGUKxN"
   },
   "source": [
    "Nama: \n",
    "<br>\n",
    "NPM: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T04:36:55.393550Z",
     "start_time": "2021-02-21T04:36:55.379838Z"
    },
    "id": "pWLsXmD0UKxN"
   },
   "source": [
    "Pada tutorial ini, kita akan melakukan:\n",
    "1. Pengumpulan data dari Twitter menggunakan Tweepy (dengan Twitter Search API), Twint, dan Twecoll. Terdapat dua jenis data yang akan kita kumpulkan, yaitu data tekstual tweet dan data network.\n",
    "2. Pengumpulan data dari suatu (<i>Web scraping</i>).\n",
    "3. Pengumpulan data dari Linkedin.\n",
    "\n",
    "Sebelum memulai tutorial, kita akan menginstal <i>library</i> yang kita perlukan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i2yDZrcpUKxO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tweepy) (2.22.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from tweepy) (1.12.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Collecting twint\n",
      "  Cloning https://github.com/twintproject/twint.git (to revision origin/master) to c:\\users\\asus\\appdata\\local\\temp\\pip-install-zxy8t38p\\twint_b42233e7068048c4b6e256423a0c3017\n",
      "Requirement already satisfied: aiohttp in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from twint) (3.7.4)\n",
      "Requirement already satisfied: aiodns in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from twint) (2.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from twint) (4.8.0)\n",
      "Requirement already satisfied: cchardet in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from twint) (2.1.7)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from twint) (0.6)\n",
      "Requirement already satisfied: elasticsearch in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from twint) (7.11.0)\n",
      "Requirement already satisfied: pysocks in c:\\users\\asus\\anaconda3\\lib\\site-packages (from twint) (1.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\anaconda3\\lib\\site-packages (from twint) (0.25.1)\n",
      "Requirement already satisfied: aiohttp_socks in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from twint) (0.6.0)\n",
      "Requirement already satisfied: schedule in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from twint) (1.0.0)\n",
      "Requirement already satisfied: geopy in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from twint) (2.1.0)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from twint) (0.1.11)\n",
      "Requirement already satisfied: googletransx in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from twint) (2.4.2)\n",
      "Requirement already satisfied: pycares>=3.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from aiodns->twint) (3.1.1)\n",
      "Requirement already satisfied: cffi>=1.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pycares>=3.0.0->aiodns->twint) (1.12.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\anaconda3\\lib\\site-packages (from cffi>=1.5.0->pycares>=3.0.0->aiodns->twint) (2.19)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from aiohttp->twint) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from aiohttp->twint) (3.7.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from aiohttp->twint) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from aiohttp->twint) (3.0.1)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->twint) (3.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from aiohttp->twint) (19.2.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp->twint) (2.8)\n",
      "Requirement already satisfied: python-socks[asyncio]>=1.2.2 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from aiohttp_socks->twint) (1.2.2)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from beautifulsoup4->twint) (1.9.3)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from elasticsearch->twint) (1.24.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\anaconda3\\lib\\site-packages (from elasticsearch->twint) (2019.9.11)\n",
      "Requirement already satisfied: geographiclib<2,>=1.49 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from geopy->twint) (1.50)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from googletransx->twint) (2.22.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->twint) (1.16.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->twint) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pandas->twint) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil>=2.6.1->pandas->twint) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/twintproject/twint.git 'C:\\Users\\ASUS\\AppData\\Local\\Temp\\pip-install-zxy8t38p\\twint_b42233e7068048c4b6e256423a0c3017'\n",
      "  WARNING: Did not find branch or tag 'origin/master', assuming revision or ref.\n",
      "  Running command git checkout -q origin/master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from selenium) (1.24.2)\n",
      "Requirement already satisfied: bs4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from bs4) (4.8.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (1.9.3)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests) (1.24.2)\n",
      "Requirement already satisfied: linkedin_scraper in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.7.2)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from linkedin_scraper) (2.22.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\asus\\anaconda3\\lib\\site-packages (from linkedin_scraper) (4.4.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\asus\\anaconda3\\lib\\site-packages (from linkedin_scraper) (3.141.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->linkedin_scraper) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->linkedin_scraper) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->linkedin_scraper) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->linkedin_scraper) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "!pip install --user --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n",
    "!pip install selenium\n",
    "!pip install bs4\n",
    "!pip install requests\n",
    "!pip install linkedin_scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjAUoMyFUKxO"
   },
   "source": [
    "# Data Collection - Twitter (Tweepy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6v-dSL4UKxO"
   },
   "source": [
    "## Twitter Search API\n",
    "\n",
    "Pengumpulan data tekstual <i>tweet</i> dapat dilakukan dengan menggunakan Twitter API. Terdapat dua kategori API Twitter, yaitu <i>Twitter Search API</i> dan <i>Twitter Streaming API</i>. Perbedaan mendasar dari kedua jenis API tersebut adalah pada waktu data <i>tweet</i> yang akan dikumpulkan. <i>Twitter Search API</i> mengambil data <i>past tweet</i>, yaitu data <i>tweet</i> dengan kriteria tertentu yang diposting pada rentang waktu H-7 hari sampai dengan waktu <i>crawling</i>. Sementara itu, <i>Twitter Streaming API</i> akan mengambil data secara <i>real-time</i> pada saat program <i>crawling</i> dijalankan. Untuk memanggil kedua API tersebut, akan digunakan <i>library</i> Tweepy.\n",
    "\n",
    "Langkah-langkah untuk melakukan proses <i>search tweet data</i> adalah sebagai berikut:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnzyn-IFUKxP"
   },
   "source": [
    "### Import library\n",
    "\n",
    "<b>Code 1</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T04:47:58.472270Z",
     "start_time": "2021-02-21T04:47:56.733683Z"
    },
    "id": "fBkvpuFnUKxP"
   },
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gk-XSFsUUKxP"
   },
   "source": [
    "### Access token variables\n",
    "\n",
    "Pertama-tama, kita harus mendaftarkan akun Twitter kita agar dapat menggunakan API. Setelah itu, Twitter akan memberikan `consumer_key`, `consumer_secret`, `access_token`, dan `access_token_secret`. Token tersebut wajib dimasukkan ke dalam parameter penggunaan Tweepy. \n",
    "\n",
    "<b>Code 2</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T04:47:59.770783Z",
     "start_time": "2021-02-21T04:47:59.757820Z"
    },
    "id": "wG6tx5ySUKxP"
   },
   "outputs": [],
   "source": [
    "consumer_key = \"IaGI9Y9bc33VARK5K4OqgCmNF\" # masukkan consumer_key Anda\n",
    "consumer_secret = \"IlzLWseGtVxb2ghlBYY125Fq2yPISiAPui3g31kHRbPSRWKJ17\" # masukkan consumer_secret Anda\n",
    "access_token = \"947706038-co0h4yFWIylo67AQzqfP1RyJTURaa4CWXmDrhNF3\" # masukkan access_token Anda\n",
    "access_token_secret = \"zhI2DKCJU8rai4gaXQTQItcVmmrC8UnuhN95sLAXtzaX4\" # masukkan access_token_secret Anda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X729BLHFUKxQ"
   },
   "source": [
    "### Setting authentication dan connection\n",
    "\n",
    "Pada tahap ini, kita membuat <i>extractor</i> yang berisi pengaturan autentifikasi dan koneksi berdasarkan <i>key</i> dan <i>token</i> yang sudah diberikan.\n",
    "\n",
    "<b>Code 3</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T04:48:01.479405Z",
     "start_time": "2021-02-21T04:48:01.472266Z"
    },
    "id": "eODUhvoEUKxQ"
   },
   "outputs": [],
   "source": [
    "def twitter_setup():\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    return api\n",
    " \n",
    "extractor = twitter_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEYWGurqUKxQ"
   },
   "source": [
    "### Crawling tweet berdasarkan query \n",
    "\n",
    "Pada contoh <b>Code 4</b> di bawah ini, <i>tweet</i> yang ingin dicari adalah 10 <i>tweet</i> yang mengandung kata `pemilu` dan berbahasa Indonesia. Parameter `extended` menunjukkan bahwa kita ingin mengambil seluruh teks pada <i>tweet</i> dan tidak hanya terbatas pada 140 karakter saja. Parameter `result_type='recent'` menunjukkan bahwa <i>tweet</i> yang kita ambil merupakan <i>tweet</i> terbaru. Terdapat jenis `result_type` yang lain, misalnya `popular` dan `mixed` (menggabungkan <i>tweet</i> terbaru dan populer). \n",
    "\n",
    "<b>Code 4</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T04:48:34.979669Z",
     "start_time": "2021-02-21T04:48:33.564926Z"
    },
    "id": "mMEDyJCYUKxR",
    "outputId": "b3306b61-38aa-447e-f69f-c9be10b41cbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah tweet yang berhasil didapatkan: 10.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = extractor.search(\n",
    "    q=\"pemilu\", lang=\"id\", result_type=\"recent\", count=10, tweet_mode=\"extended\")\n",
    "\n",
    "print(\"Jumlah tweet yang berhasil didapatkan: {}.\\n\".format(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T04:53:08.204314Z",
     "start_time": "2021-02-21T04:53:08.197717Z"
    },
    "id": "oNE5yfEEUKxR",
    "outputId": "874467ec-064f-4730-ca5e-b04a2ba53bf7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Tue Apr 20 01:34:27 +0000 2021',\n",
       " 'id': 1384319518105751565,\n",
       " 'id_str': '1384319518105751565',\n",
       " 'full_text': '@tandaserusaja Wah ngegas dari kemarin. Mangkanya waktu pemilu kemarin dengar apa kata sarjana politik, nda usah fanatik sama salah satu calon. Wkwkwkwkw',\n",
       " 'truncated': False,\n",
       " 'display_text_range': [15, 153],\n",
       " 'entities': {'hashtags': [],\n",
       "  'symbols': [],\n",
       "  'user_mentions': [{'screen_name': 'tandaserusaja',\n",
       "    'name': 'tandaserusaja',\n",
       "    'id': 515569313,\n",
       "    'id_str': '515569313',\n",
       "    'indices': [0, 14]}],\n",
       "  'urls': []},\n",
       " 'metadata': {'iso_language_code': 'in', 'result_type': 'recent'},\n",
       " 'source': '<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>',\n",
       " 'in_reply_to_status_id': 1384318919662440450,\n",
       " 'in_reply_to_status_id_str': '1384318919662440450',\n",
       " 'in_reply_to_user_id': 515569313,\n",
       " 'in_reply_to_user_id_str': '515569313',\n",
       " 'in_reply_to_screen_name': 'tandaserusaja',\n",
       " 'user': {'id': 2272471933,\n",
       "  'id_str': '2272471933',\n",
       "  'name': 'üçÄ',\n",
       "  'screen_name': 'mamiyueo',\n",
       "  'location': '',\n",
       "  'description': \"Ult li albi bissaraha \\nHayya nab'idil karaha \\nSyakkireena a' kulli ni'ma\\nBa' ideena anil fattana\\n| @tandaserusaja wifi üòÇ\",\n",
       "  'url': None,\n",
       "  'entities': {'description': {'urls': []}},\n",
       "  'protected': False,\n",
       "  'followers_count': 64,\n",
       "  'friends_count': 218,\n",
       "  'listed_count': 1,\n",
       "  'created_at': 'Thu Jan 02 04:18:38 +0000 2014',\n",
       "  'favourites_count': 838,\n",
       "  'utc_offset': None,\n",
       "  'time_zone': None,\n",
       "  'geo_enabled': True,\n",
       "  'verified': False,\n",
       "  'statuses_count': 4097,\n",
       "  'lang': None,\n",
       "  'contributors_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'is_translation_enabled': False,\n",
       "  'profile_background_color': 'C0DEED',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_tile': False,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/1378990245773942787/TPHO13yD_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1378990245773942787/TPHO13yD_normal.jpg',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/2272471933/1580653601',\n",
       "  'profile_link_color': '1DA1F2',\n",
       "  'profile_sidebar_border_color': 'C0DEED',\n",
       "  'profile_sidebar_fill_color': 'DDEEF6',\n",
       "  'profile_text_color': '333333',\n",
       "  'profile_use_background_image': True,\n",
       "  'has_extended_profile': False,\n",
       "  'default_profile': True,\n",
       "  'default_profile_image': False,\n",
       "  'following': False,\n",
       "  'follow_request_sent': False,\n",
       "  'notifications': False,\n",
       "  'translator_type': 'none'},\n",
       " 'geo': None,\n",
       " 'coordinates': None,\n",
       " 'place': None,\n",
       " 'contributors': None,\n",
       " 'is_quote_status': False,\n",
       " 'retweet_count': 0,\n",
       " 'favorite_count': 0,\n",
       " 'favorited': False,\n",
       " 'retweeted': False,\n",
       " 'lang': 'in'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contoh data\n",
    "tweets[9]._json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pox9VGtJUKxS"
   },
   "source": [
    "### <i>Crawling tweet</i> dari <i>username</i> spesifik\n",
    "\n",
    "Untuk mendapatkan <i>tweet</i> dari <i>username</i> spesifk, digunakan fungsi `user_timeline` dengan parameter `screen_name`. Pada contoh ini akan diambil 10 <i>tweet</i> terkini dari @Univ_Indonesia. \n",
    "\n",
    "<b>Code 5</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T04:53:48.761480Z",
     "start_time": "2021-02-21T04:53:46.785558Z"
    },
    "id": "1ZRhMNDTUKxS",
    "outputId": "8295c3d7-4015-4a4f-995f-12dc1cc33c93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah tweet dari @Univ_Indonesia yang berhasil didapatkan: 10.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets2 = extractor.user_timeline(\n",
    "    screen_name=\"@univ_indonesia\", lang=\"id\", result_type=\"recent\", count=10, tweet_mode=\"extended\")\n",
    "\n",
    "print(\"Jumlah tweet dari @Univ_Indonesia yang berhasil didapatkan: {}.\\n\".format(len(tweets2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T04:53:48.780906Z",
     "start_time": "2021-02-21T04:53:48.768465Z"
    },
    "id": "nws7clw6UKxS",
    "outputId": "62bcfc10-db56-4949-9782-53e5c0886f6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(_api=<tweepy.api.API object at 0x000001EBC1651588>, _json={'created_at': 'Fri Apr 16 05:50:14 +0000 2021', 'id': 1382934339835498498, 'id_str': '1382934339835498498', 'full_text': '@vanyulio Wah kok mirip dengan yang di foto ya, Kak?', 'truncated': False, 'display_text_range': [10, 52], 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'vanyulio', 'name': 'Van Yulio', 'id': 372160621, 'id_str': '372160621', 'indices': [0, 9]}], 'urls': []}, 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>', 'in_reply_to_status_id': 1382932605562023938, 'in_reply_to_status_id_str': '1382932605562023938', 'in_reply_to_user_id': 372160621, 'in_reply_to_user_id_str': '372160621', 'in_reply_to_screen_name': 'vanyulio', 'user': {'id': 74646907, 'id_str': '74646907', 'name': 'UniversitasIndonesia', 'screen_name': 'univ_indonesia', 'location': '', 'description': 'Official account of Universitas Indonesia. Delivering news, events, and stories on campus or something you should know. Add us on LINE @univ_indonesia', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 1242595, 'friends_count': 94, 'listed_count': 577, 'created_at': 'Wed Sep 16 03:57:13 +0000 2009', 'favourites_count': 682, 'utc_offset': None, 'time_zone': None, 'geo_enabled': True, 'verified': False, 'statuses_count': 35304, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'FFFFFF', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1364419729096511488/_DzbKx15_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1364419729096511488/_DzbKx15_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/74646907/1614137989', 'profile_link_color': 'FFCC4D', 'profile_sidebar_border_color': 'FFFFFF', 'profile_sidebar_fill_color': 'FFD608', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': True, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'regular', 'withheld_in_countries': []}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 0, 'favorite_count': 0, 'favorited': False, 'retweeted': False, 'lang': 'in'}, created_at=datetime.datetime(2021, 4, 16, 5, 50, 14), id=1382934339835498498, id_str='1382934339835498498', full_text='@vanyulio Wah kok mirip dengan yang di foto ya, Kak?', truncated=False, display_text_range=[10, 52], entities={'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'vanyulio', 'name': 'Van Yulio', 'id': 372160621, 'id_str': '372160621', 'indices': [0, 9]}], 'urls': []}, source='Twitter for iPhone', source_url='http://twitter.com/download/iphone', in_reply_to_status_id=1382932605562023938, in_reply_to_status_id_str='1382932605562023938', in_reply_to_user_id=372160621, in_reply_to_user_id_str='372160621', in_reply_to_screen_name='vanyulio', author=User(_api=<tweepy.api.API object at 0x000001EBC1651588>, _json={'id': 74646907, 'id_str': '74646907', 'name': 'UniversitasIndonesia', 'screen_name': 'univ_indonesia', 'location': '', 'description': 'Official account of Universitas Indonesia. Delivering news, events, and stories on campus or something you should know. Add us on LINE @univ_indonesia', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 1242595, 'friends_count': 94, 'listed_count': 577, 'created_at': 'Wed Sep 16 03:57:13 +0000 2009', 'favourites_count': 682, 'utc_offset': None, 'time_zone': None, 'geo_enabled': True, 'verified': False, 'statuses_count': 35304, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'FFFFFF', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1364419729096511488/_DzbKx15_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1364419729096511488/_DzbKx15_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/74646907/1614137989', 'profile_link_color': 'FFCC4D', 'profile_sidebar_border_color': 'FFFFFF', 'profile_sidebar_fill_color': 'FFD608', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': True, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'regular', 'withheld_in_countries': []}, id=74646907, id_str='74646907', name='UniversitasIndonesia', screen_name='univ_indonesia', location='', description='Official account of Universitas Indonesia. Delivering news, events, and stories on campus or something you should know. Add us on LINE @univ_indonesia', url=None, entities={'description': {'urls': []}}, protected=False, followers_count=1242595, friends_count=94, listed_count=577, created_at=datetime.datetime(2009, 9, 16, 3, 57, 13), favourites_count=682, utc_offset=None, time_zone=None, geo_enabled=True, verified=False, statuses_count=35304, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='FFFFFF', profile_background_image_url='http://abs.twimg.com/images/themes/theme1/bg.png', profile_background_image_url_https='https://abs.twimg.com/images/themes/theme1/bg.png', profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1364419729096511488/_DzbKx15_normal.jpg', profile_image_url_https='https://pbs.twimg.com/profile_images/1364419729096511488/_DzbKx15_normal.jpg', profile_banner_url='https://pbs.twimg.com/profile_banners/74646907/1614137989', profile_link_color='FFCC4D', profile_sidebar_border_color='FFFFFF', profile_sidebar_fill_color='FFD608', profile_text_color='333333', profile_use_background_image=True, has_extended_profile=True, default_profile=False, default_profile_image=False, following=True, follow_request_sent=False, notifications=False, translator_type='regular', withheld_in_countries=[]), user=User(_api=<tweepy.api.API object at 0x000001EBC1651588>, _json={'id': 74646907, 'id_str': '74646907', 'name': 'UniversitasIndonesia', 'screen_name': 'univ_indonesia', 'location': '', 'description': 'Official account of Universitas Indonesia. Delivering news, events, and stories on campus or something you should know. Add us on LINE @univ_indonesia', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 1242595, 'friends_count': 94, 'listed_count': 577, 'created_at': 'Wed Sep 16 03:57:13 +0000 2009', 'favourites_count': 682, 'utc_offset': None, 'time_zone': None, 'geo_enabled': True, 'verified': False, 'statuses_count': 35304, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'FFFFFF', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1364419729096511488/_DzbKx15_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1364419729096511488/_DzbKx15_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/74646907/1614137989', 'profile_link_color': 'FFCC4D', 'profile_sidebar_border_color': 'FFFFFF', 'profile_sidebar_fill_color': 'FFD608', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': True, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'regular', 'withheld_in_countries': []}, id=74646907, id_str='74646907', name='UniversitasIndonesia', screen_name='univ_indonesia', location='', description='Official account of Universitas Indonesia. Delivering news, events, and stories on campus or something you should know. Add us on LINE @univ_indonesia', url=None, entities={'description': {'urls': []}}, protected=False, followers_count=1242595, friends_count=94, listed_count=577, created_at=datetime.datetime(2009, 9, 16, 3, 57, 13), favourites_count=682, utc_offset=None, time_zone=None, geo_enabled=True, verified=False, statuses_count=35304, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='FFFFFF', profile_background_image_url='http://abs.twimg.com/images/themes/theme1/bg.png', profile_background_image_url_https='https://abs.twimg.com/images/themes/theme1/bg.png', profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1364419729096511488/_DzbKx15_normal.jpg', profile_image_url_https='https://pbs.twimg.com/profile_images/1364419729096511488/_DzbKx15_normal.jpg', profile_banner_url='https://pbs.twimg.com/profile_banners/74646907/1614137989', profile_link_color='FFCC4D', profile_sidebar_border_color='FFFFFF', profile_sidebar_fill_color='FFD608', profile_text_color='333333', profile_use_background_image=True, has_extended_profile=True, default_profile=False, default_profile_image=False, following=True, follow_request_sent=False, notifications=False, translator_type='regular', withheld_in_countries=[]), geo=None, coordinates=None, place=None, contributors=None, is_quote_status=False, retweet_count=0, favorite_count=0, favorited=False, retweeted=False, lang='in')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contoh data\n",
    "tweets2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGXBVbScUKxT"
   },
   "source": [
    "### Menyimpan hasil <i>crawl tweet</i> ke CSV\n",
    "\n",
    "Langkah selanjutnya adalah menyimpan <i>tweet</i> yang diambil ke dalam <i>file</i> csv. Pada percobaan ini, akan disimpan <i>tweet</i> hasil dari pencarian pada <b>Code 4</b>. \n",
    "\n",
    "<b>Code 6</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T05:29:27.657394Z",
     "start_time": "2021-02-21T05:29:27.643242Z"
    },
    "id": "aukXwUuHUKxT"
   },
   "outputs": [],
   "source": [
    "fw = open(\"hasil_tweet_c5.csv\", \"w\", encoding =\"utf-8\")\n",
    "for tweet in tweets:\n",
    "    # mendapatkan tweet > 140 chars\n",
    "    text = tweet.full_text\n",
    "    \n",
    "    # mendapatkan bio author\n",
    "    bio = tweet.user.description\n",
    "\n",
    "    # mendapatkan tweet asli yang diretweet\n",
    "    if hasattr(tweet, \"retweeted_status\"):\n",
    "        text = tweet.retweeted_status.full_text\n",
    "        \n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # menuliskan ke dalam file .csv\n",
    "    fw.write(text+ \" \"+ \", Bio author : \" + bio +\"\\n\")\n",
    "\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgxWuTtzUKxT"
   },
   "source": [
    "Selain parameter-parameter diatas, terdapat pula parameter lain misalnya geocode, since_id dan sebagainya. Daftar lengkap parameter <i>Twitter Search API</i> dan format <i>output request</i> bisa dilihat di <a href='https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html'>API references</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgNcApujUKxT"
   },
   "source": [
    "## Twitter Streaming API\n",
    "\n",
    "Tweet yang dikumpulkan menggunakan <i>Twitter Streaming API</i> merupakan data <i>tweet</i> yang bersifat <i>real time</i>. Langkah-langkah melakukan pengumpulan data dengan <i>Twitter Streaming API</i> adalah sebagai berikut:\n",
    "\n",
    "<b>Code 7</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T05:13:23.421201Z",
     "start_time": "2021-02-21T05:13:23.415749Z"
    },
    "id": "7ynevzWYUKxT"
   },
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    " \n",
    "consumer_key = 'IaGI9Y9bc33VARK5K4OqgCmNF'\n",
    "consumer_secret = 'IlzLWseGtVxb2ghlBYY125Fq2yPISiAPui3g31kHRbPSRWKJ17'\n",
    "access_token = '947706038-co0h4yFWIylo67AQzqfP1RyJTURaa4CWXmDrhNF3'\n",
    "access_token_secret = 'zhI2DKCJU8rai4gaXQTQItcVmmrC8UnuhN95sLAXtzaX4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fz2W18h6UKxU"
   },
   "source": [
    "### Membuat StreamListener\n",
    "\n",
    "Kelas `StreamListener` akan memberikan informasi mengenai status <i>request data</i> Twitter yang kita cari. Pada kelas ini, terdapat dua fungsi yaitu fungsi on_data() yang mengembalikan data yang kita <i>request</i> dan fungsi `on_error()` yang memberikan informasi jika <i>request</i> gagal.\n",
    "\n",
    "Di dalam fungsi `on_data()`, akan didapatkan keluaran <i>json string</i> yang berisi data hasil <i>request</i>. Untuk mendapatkan komponen <i>tweet</i> yang diinginkan, kita perlu melakukan <i>parsing</i> data json tersebut. Sementara itu, fungsi `on_error()` mengembalikan pesan <i>error</i> apabila terdapat <i>error</i> pada <i>request</i>.\n",
    "\n",
    "<b>Code 8</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T05:33:23.495256Z",
     "start_time": "2021-02-21T05:33:23.481050Z"
    },
    "id": "4fXFj2I9UKxU"
   },
   "outputs": [],
   "source": [
    "class MyStreamListener(StreamListener):\n",
    "    def on_data(self, data):\n",
    "        # keluaran data dalam format json string\n",
    "        all_data = json.loads(data)\n",
    "        if \"extended_tweet\" in all_data:\n",
    "            fw = open(\"hasil_tweet_c9.csv\", \"a\", encoding =\"utf-8\")\n",
    "            text = all_data[\"extended_tweet\"][\"full_text\"]\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            time_created = all_data[\"created_at\"]\n",
    "            fw.write(\"Time created :\" + time_created + \", text : \" + text + \" \" +  \"\\n\")\n",
    "            print(text)\n",
    "            print('-'*60)\n",
    "    def on_error(self, status):\n",
    "        print(status.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9aLijo4UKxV"
   },
   "source": [
    "### Membuat stream\n",
    "\n",
    "Fungsi `Stream` digunakan untuk melakukan <i>request streaming</i> berdasarkan autorisasi dari token API yang kita miliki. Pada fungsi `API()` terdapat parameter tambahan `wait_on_rate_limit=True` dan `wait_on_rate_limit_notify=True` agar program tetap berjalan dan menunggu selama <i>request</i> sudah melebihi <i>limit</i>.\n",
    "\n",
    "Untuk standar parameter filter yang lain dapat dilihat di halaman <a href='https://developer.twitter.com/en/docs/tweets/filter-realtime/guides/basic-stream-parameters.html'>dokumentasi</a> Twitter.\n",
    "\n",
    "<b>Code 9</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T05:33:31.771307Z",
     "start_time": "2021-02-21T05:33:24.289705Z"
    },
    "id": "nASMLmH0UKxV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@jugunagan nikahin suka susu opet jadi kek menggilan ikan hias makan buku kek gorneg kerupuk makan tai lalat melaut ke bajak kemana mana makan santai minum goreng rebuh rambut\n",
      "------------------------------------------------------------\n",
      "@ligfiredehp Gue suka nontonin tanboy kun karna dia gapernah lupa berdoa sebelum makan wkwk, kalo korea paling demen sma hamzy btw semangat puasanya gue sih lg g puasaüòÇ\n",
      "------------------------------------------------------------\n",
      "Cinnamon tak maneh, how can cinnamon rasa maneh? I sukaa bauu diaaaa, makan pun sedap tapiil dia tak manis.... Ke dia memang manis tapi bukan manis sangat?\n",
      "------------------------------------------------------------\n",
      "Dia punya quote macam dulu susah menjala ikan tempuh ombak, redah hujan bawa grabfood, jalan kaki pergi klinik kesihatan check up baby sebab takda transport, tak makan berhari hri sebab takda duit. Padahal lahir lahir dah atas kayangan lol what a joke\n",
      "------------------------------------------------------------\n",
      "@ryuxellashin karena warna warna makanan yang kita makan kecampur di sistem pencernaan kita dan alhasil menjadi warna coklat, tapi nggak tau juga sih karena aku ngasal\n",
      "------------------------------------------------------------\n",
      "@ZOO_FESS Beberapa hari lalu kucing yang suka aku kasih makan ketabrak, sebelum aku belanja sayur si kucing masih deketin minta makan pas pulang udah mati di pinggir jalan beneran orang tega banget üò≠üò≠\n",
      "------------------------------------------------------------\n",
      "@ZPlhiv Biasa mula2 pengambilan rasa MCM tu lama kelamaan dah tiada dah rasa mual dan loya walaupun selepas ambil ubat(itu pengalaman sy la 2thn makan ubat)\n",
      "------------------------------------------------------------\n",
      "kelakar bila nak lawan dengan tandatangan. maki hamun pun dia tak makan, ni nak guna tandatangan.   laksamana bentan datang menyembah, merendah diri walaupun gagah, raja adil raja disembah, raja zalim raja disanggah.\n",
      "------------------------------------------------------------\n",
      "@khairizulfadhli Kau ni dah kenapa? makan nasi ke benda lain? Asyik duduk kutuk orang lain je. Perengai bapok la kau mok. Kau jangan turun Muar. Aku jumpa kau je terus ludah atas muka kau.\n",
      "------------------------------------------------------------\n",
      "@JonginWife91 One of the alternative is vitro human skin equivalent. Dia buat dari sel to tisu to skin. Tapi makan cost sikit sbb kene buat kat lab, kalau pakai haiwan diorang just tangkap je. Zana tahu benda ni pun sbb sem lepas ade buat small speech about animal testing üòÖ https://t.co/twmPyaPj02\n",
      "------------------------------------------------------------\n",
      "„Ö§ „Ö§ \"Asha mau titip Tata dulu di sini sebab mau jenguk Zea di Mayapada sekalian ada rapat sama klien sampe sore,\"  \"Oalah gitu, yowes gak apa-apa. Si Zea kenapa itu kok bisa masuk rumah sakit?\"  \"Halah, biasalah anak muda. Coba-coba makan, taunya alerginya kambuh itu,\" „Ö§ „Ö§\n",
      "------------------------------------------------------------\n",
      "awalnya bikin cerita kejadian pindah Agama dg di bumbuin kebohongan bla....blaa...blaa..utk bahan supaya pendengar terhipnotis..sehingga tersebar dari mulut ke mulut terus terkenal keenaakan utk cari makan lewat ngehoax jadi Mualaf..\n",
      "------------------------------------------------------------\n",
      "Setelah berhasil puasa 2hari after sakit, hari ke-3 kembali tidak berpuasa. Perut krucuk\" kek dikrues dari malem pdhl udah makan, udah minum obat juga.\n",
      "------------------------------------------------------------\n",
      "Serial Amalan Utama Ajaran Rasulullah SAW #13  Rasulullah Shallalahu 'alaihi wa sallam pernah bersabda: \"Yang terbaik dari kalian adalah yang memberi makan dan menjawab salam.\"  Musnad Ahmad Hadits No. 22800 https://t.co/gbeXYKkD2r\n",
      "------------------------------------------------------------\n",
      "Bagaimana respons MUI Kota Serang terkait polemik larangan warung makan buka siang hari? - VIDEO: Pemkot Serang Larang Warung Makan Buka https://t.co/NcqxqaMhJs\n",
      "------------------------------------------------------------\n",
      "@Hysirii Ngapain makan rambut :(  Eh iya buat yang di Bandung yang mau tes HIV dan IMS bisa chat aku yah. Ada Tes VCT Gratis program pemerintah. Dapat kondom gratis juga. Yuk sayangi diri dan pasangan. https://t.co/2dtT6Vbq3q\n",
      "------------------------------------------------------------\n",
      "@ampaskayujati @Hariang_ @DivHumas_Polri iya, bang... demi Allah gw sampe sumpah¬≤in tuh oknum... biar mampus pas makan duit gw bareng keluarganya...\n",
      "------------------------------------------------------------\n",
      "Kalo kenangan buruk susah banget dilupain, giliran nginget-nginget kemaren makan apa sampe alergi begini kagak ada yg nyantol satu pun ini.... YaAllaaaah üò≠\n",
      "------------------------------------------------------------\n",
      "@damdamie_ Gue juga ada stray cat gt dari bayi pas emanya lahir dia disini sampe gede gini gue urus kasih makan, bokap gue yg mandiin tapi gak tinggal di rumah di luar rmh, tuh lucu kan https://t.co/XtotVdg7xH\n",
      "------------------------------------------------------------\n",
      "@ninthoddsmel Wah, dia terkesan.  \"Rekan-rekanku selalu mengeluh kalau aku makan cup ramen. Terima kasih sudah menemani.\" sambil seruput-seruput. Tau-tau habis.\n",
      "------------------------------------------------------------\n",
      "Jadi kucing gue tuh kek lagi mam biskuit terus gue kek mainan gt sama kucingny biskuitny w masukin mulut seakan2 gue ngajak meng mainan pocky game asu tp namany biskuit yh lama2 ad yg lumer sdkit dlm mulut ak makan biskuit meng üò∞\n",
      "------------------------------------------------------------\n",
      "@firdausAzlan13 we have dumb government, of course kita stuck here in Selangor for another year.  yes, kalau cuti raya, kena cari yg best and ada makan skali\n",
      "------------------------------------------------------------\n",
      "@biyarin_ @yourpetsfess Hai aku sendernya. Udah kak. Udh dibiarin main sepuasnya diluar. Kebetulan disekitar rumahku jarang ada kucing liar. Disuruh pulang buat makan aja krn emng gak mau makan samsek. Pdhl waktu opname dia heboh pgn plg pas plg kambuh lagi.\n",
      "------------------------------------------------------------\n",
      "jadi cat lovers sejak dalam kandungan ibu, kakek nenek om tante semua cat lovers, setiap bulan untuk makan pasir mereka bisa diangka 5jt belum biaya vaksin, steril, berobat, operasi dll, engga ada tuh menyesal dan teriak bilang uang habis untuk mereka.\n",
      "------------------------------------------------------------\n",
      "@ieoeelmik kehitungggg wkwkk ayo kita kasih makan buat tyong!  also aduh gue beneran kepikiran anjir gimana kalo tyong beneran ngikutin jejak 3racha .... SC dulu tiap 2 minggu sekali, baru bikin mini album tahun depannya\n",
      "------------------------------------------------------------\n",
      "@psiyong Astaga ga masuk mentab.  .  Betul, belum lagi kalau tipe yang susah makan. Pas bukber taunya bukan makanan yang disuka jadi cuma makan sedikit. Harus beli lagi T^T\n",
      "------------------------------------------------------------\n",
      "@ntsana_ dlu th 2016 juga waktu masi kelas 3 sma. aku tinggal berdua sm mama, lagi gk ada duit sm sekali cuma punya 5rb mama kasi ke aku buat bekel sekolah. disekolah gk jajan sm sekali cuma bawa minum dan temenku baik2 jd dijajanin. nyampe rumah mama tidur , kayanya blm makan apa2 -cont\n",
      "------------------------------------------------------------\n",
      "Aku kalau kakak aku dengan kakak ipar aku ada dekat rumah.. tengak dah pukul 7 aku cakap ‚Äúboleh dah kita ni standby‚Äù diorang yang buat aku lari üòÖüòÇ\n",
      "------------------------------------------------------------\n",
      "@okkyvinasty  hai ka ,mohon maaf sebelumnya perihal paket tersebut di system kami tidak bisa kami temukan ya ka ,sesuai prosedur perusahaan kami ,makan akan kami bantu proses pegantian paket tersebut ke pihak pengirim atau mark.. https://t.co/509nx0Q3g7\n",
      "------------------------------------------------------------\n",
      "@fessthai nder, demi deh nemu papo pacaran disawah makan diempang juga gue mau nder, PACARAN PAKE SUPRA MODAL GEDEBAGE - CIAMIS juga ak rela nder, sIAPA JGA YG TAK MAU MODELAN PAPO BGNI ampunn\n",
      "------------------------------------------------------------\n",
      "Durian Ori Moonkek.‚ù§Ô∏è. Antara flavour yang top dan premium dekat MimiLala ni tau.. Sangatlah rugi kalo tak try..rasanya bila masuk mulut mmg meltinggg ü§§ü§§ Tekstur sgt moist dan lembut.. less sweet pulak tau..kanak2 pun boleh makan tau  üëçüëçü•∞ https://t.co/0FShi5REax\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pembagian Ta'zil Makan Nasi Box untuk Berbuka Puasa kepada Warga Masyarakat slum Area Di Jl. Peninggaran Barat II  RT. 10 RW. 11 Kel. Keb Lama Utara  Kec. Kebayoran Lama Jakarta Selatan Oleh IPTU SUTIJAN (Kapolsubsektor Permata Hjau) Dan Binmas AIPTU ROHMADI sebanyak 35 bungkus. https://t.co/GhsMzTJY8k\n",
      "------------------------------------------------------------\n",
      "Pets! Ada yg pernah gak pas mengnya sakit trs malemnya dia super lemes trs kalian ngerasa badannya mulai dingin. Aku pernah mengku kayak gt, sblm tdr aku suapin makan sm madu. Tp paginya ttp ga ketolongüò≠ kalo kyk gitu, gmn ya caranya supaya dia bisa bertahan?üò≠\n",
      "------------------------------------------------------------\n",
      "Aku nak ludah je lah pegawai @KKMPutrajaya yg macam anjing ni. Kes lopak takde tindakan tiba2 dgn Student kau bukan makan menyalak Ye\n",
      "------------------------------------------------------------\n",
      "Dulu, ada anak twitter tau alamat kantor dan sok ngirim surprise gofood. Karna ngerasa gak beli. Terus temen aing yang makan. Besok nya orangnya nanya üòÇüòÇ Lah kan gak tau ya, saya anaknya jujur, gak saya makan wkwkw Besoknya dia ngirim lagi, tapi konfirmasi, akhirnya diterima üòÇ\n",
      "------------------------------------------------------------\n",
      "@susipudjiastuti Apanya yg salah jika uang makan pekerja 7,5 miliar perhari bu Sus?? bukankah uang yg berputar tersebut berarti perekonomian di masyarakat sekitarnya bisa bertambah menjadi lebih baik?? sekali2 liat dari segi positifnya dong Bu??\n",
      "------------------------------------------------------------\n",
      "Sebenarnya bulan puasa ni makan kurma pun dah cukup masa bersahur untuk kekal kenyang sehingga berbuka.. Tapi kena pandai pilih kurma.. Contohnya, ayam masak kurma, daging masak kurma, kambing masak kurma..   Good luck.. ü§£\n",
      "------------------------------------------------------------\n",
      "Korang, baca tweet ni. Jangan main hp je tak tolong isteri/ mak karang diorg tak masak baru tahu üòÜüòÜ  @firdaus_pitt @syafiqq_abdulla @AbdullahAmiru11 @_mieezul @Ridridzuan2\n",
      "------------------------------------------------------------\n",
      "cuma karena papa di PHK karna dampak covid dari desember, dan sekarang ngeluh \"ga mampu\" ngasih makan mereka langsung dikata katain. coba cek kerumah seperti apa kondisi mereka.\n",
      "------------------------------------------------------------\n",
      "@muannas_alaidid Sudahlah kek.. gausah ditanggapi,, terus bersabar ya. Sya cuma bisa mendoakan semoga sodara saya yg mualaf itu segera sadar dan cari makan yg halal\n",
      "------------------------------------------------------------\n",
      "Jaid keinget empus yg matinya juga gara\" gasengaja makan tikus tetangga yg diracun, gw nyesel senyesel\"nya kenapa pagi itu gak ngasi makan dia yg banyak, kenapa waktu itu dia dibolehin keluar. Pengen bgt marahin yg buang tikus sembarangan. Tapi yaudah lah akhirnya cma bisa ikhlas\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWantReadError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1821\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1822\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36m_raise_ssl_error\u001b[1;34m(self, ssl, result)\u001b[0m\n\u001b[0;32m   1621\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_ERROR_WANT_READ\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1622\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mWantReadError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1623\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0merror\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_ERROR_WANT_WRITE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWantReadError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-663d07bc1234>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtwitterStream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMyStreamListener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtwitterStream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"makan\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"in\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, follow, track, is_async, locations, stall_warnings, languages, encoding, filter_level)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'filter_level'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_level\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'delimited'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'length'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_async\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     def sitestream(self, follow, stall_warnings=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_start\u001b[1;34m(self, is_async)\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnooze_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnooze_time_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mssl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m                 \u001b[1;31m# This is still necessary, as a SSLError can actually be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36m_read_loop\u001b[1;34m(self, resp)\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m                 \u001b[0mstripped_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mline\u001b[0m \u001b[1;31m# line is sometimes None so we need to check here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstripped_line\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\streaming.py\u001b[0m in \u001b[0;36mread_line\u001b[1;34m(self, sep)\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Close the connection when no data is returned\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readinto_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_readinto_chunked\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_chunk_left\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtotal_bytes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_get_chunk_left\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    552\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# toss the CRLF at the end of the chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m                 \u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_next_chunk_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_next_chunk_size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_next_chunk_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;31m# Read the next chunk size from the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"chunk size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWantReadError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The read operation timed out'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36mwait_for_read\u001b[1;34m(sock, timeout)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msocket\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreadable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0mexpired\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \"\"\"\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mwait_for_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36mselect_wait_for_socket\u001b[1;34m(sock, read, write, timeout)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# thing.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcheck\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwcheck\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwcheck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mrready\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwready\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_retry_on_intr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrready\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mwready\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mxready\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36m_retry_on_intr\u001b[1;34m(fn, timeout)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# Modern Python, that retries syscalls by default\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_retry_on_intr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Old and broken Pythons.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True,\n",
    "          retry_count=10, retry_delay=5, retry_errors=5)\n",
    "twitterStream = Stream(api.auth, MyStreamListener())\n",
    " \n",
    "twitterStream.filter(track=[\"makan\"], languages=[\"in\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nomor 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah tweet yang berhasil didapatkan: 10.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets3 = extractor.search(\n",
    "    q=\"#Ruangguru6Tahun\", lang=\"id\", result_type=\"popular\", tweet_mode=\"extended\")\n",
    "\n",
    "print(\"Jumlah tweet yang berhasil didapatkan: {}.\\n\".format(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = open(\"hasil_tweet_populer1.csv\", \"w\", encoding =\"utf-8\")\n",
    "for tweet in tweets3:\n",
    "    # mendapatkan tweet > 140 chars\n",
    "    text = tweet.full_text\n",
    "\n",
    "    # mendapatkan tweet asli yang diretweet\n",
    "    if hasattr(tweet, \"retweeted_status\"):\n",
    "        text = tweet.retweeted_status.full_text\n",
    "        \n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # menuliskan ke dalam file .csv\n",
    "    fw.write(text +\"\\n\")\n",
    "\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nomor 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets4 = extractor.search(\n",
    "    q=\"tidur -siang\", lang=\"id\", result_type=\"popular\", count=10, tweet_mode=\"extended\")\n",
    "\n",
    "print(\"Jumlah tweet yang berhasil didapatkan: {}.\\n\".format(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = open(\"hasil_tweet_populer2.csv\", \"w\", encoding =\"utf-8\")\n",
    "for tweet in tweets4:\n",
    "    # mendapatkan tweet > 140 chars\n",
    "    text = tweet.full_text\n",
    "\n",
    "    # mendapatkan tweet asli yang diretweet\n",
    "    if hasattr(tweet, \"retweeted_status\"):\n",
    "        text = tweet.retweeted_status.full_text\n",
    "        \n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # menuliskan ke dalam file .csv\n",
    "    fw.write(text +\"\\n\")\n",
    "\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nomor 5\n",
    "\n",
    "Pada Operasi OR tepatnya pada file hasil tweet popular 1, Twitter Search mencari tweet yang mengandung kata tidur ataupun siang, sedangkan pada Operasi - hanya mencari tweet yang mengandung kata tidur dan tidak ada kata siang didalamanya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uflvMrpUKxV"
   },
   "source": [
    "# Data Collection - Twitter (Twint)\n",
    "\n",
    "Selain menggunakan Tweepy, pengumpulan data tekstual <i>tweet</i> dapat juga dilakukan dengan menggunakan <i>library</i> <a href='https://github.com/twintproject/twint'>Twint</a>. Perbedaan <i>library</i> ini dengan Tweepy adalah:\n",
    "1. Twint hanya bisa digunakan untuk mengambil data <i>past tweet</i>, namun rentang waktunya tidak terbatas, seperti Tweepy (H-7)\n",
    "2. Tidak membutuhkan <i>access token</i> Twitter API.\n",
    "\n",
    "Berikut merupakan contoh penggunaan Twint dalam melakukan <i>crawling tweet</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T07:13:23.672924Z",
     "start_time": "2021-02-21T07:13:22.868565Z"
    },
    "id": "xs0Dwv2YUKxW"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nest_asyncio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-67cc093a2eb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import library\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtwint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnest_asyncio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnest_asyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nest_asyncio'"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import twint\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T07:37:35.634233Z",
     "start_time": "2021-02-21T07:13:24.486422Z"
    },
    "id": "90IVCRvVUKxW",
    "outputId": "ed5d6c80-e82d-4528-d300-bad418aa57cb"
   },
   "outputs": [],
   "source": [
    "c = twint.Config()\n",
    "c.Search = 'banjir' # masukkan kata kunci\n",
    "c.Since = '2021-02-20 00:00:00'\n",
    "c.Until = '2021-02-20 00:05:00' \n",
    "c.Store_csv = True\n",
    "c.Output = 'banjir.csv'\n",
    "c.Hide_output = True\n",
    "\n",
    "twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3Ol0kKCUKxW"
   },
   "source": [
    "# Data Collection - Twecoll\n",
    "\n",
    "Pada tutorial kali ini, akan dibahas tentang langkah-langkah untuk mengetahui <i>friends-of-friends network</i>. Analisis <i>friends-of-friends</i> merupakan analisis yang dilakukan pada\n",
    "jaringan media sosial yang bertujuan untuk mengetahui jaringan pertemanan. Pada tutorial\n",
    "ini, akan dilakukan pengumpulan data jaringan <i>friends-of-friends</i> dari Twitter dengan\n",
    "menggunakan Twecoll.\n",
    "\n",
    "Langkah-langkah untuk untuk mengambil data pertemanan menggunakan Twecoll adalah\n",
    "sebagai berikut:\n",
    "1. Masuk ke direktori dari twecoll dan eksekusi perintah berikut:<br>\n",
    "```\n",
    "python twecoll init <USERNAME_TWITTER_TARGET>\n",
    "```\n",
    "Setelah selesai akan dihasilkan berkas .dat dengan format <USERNAME_TWITTER_TARGET>.dat\n",
    "\n",
    "2. Ketika baru dijalankan pertama kali, maka program akan meminta <i>consumer secret</i> dan\n",
    "<i>consumer key</i>. Masukkan <i>consumer secret</i> dan <i>consumer key</i> masing-masing.\n",
    "\n",
    "3. Setelah terverifikasi, program akan mengarahkan ke <i>browser</i> yang berisi PIN. Berikut\n",
    "adalah contoh halaman yang menampilkan PIN.\n",
    "<img src='images/image2.png'></img>\n",
    "Masukkan PIN yang ditampilkan di <i>browser</i> ke dalam program.\n",
    "\n",
    "4. Setelah selesai, eksekusi perintah berikut: \n",
    "```\n",
    "python twecoll fetch <USERNAME_TWITTER_TARGET>\n",
    "```\n",
    "Proses ini akan menghasilkan suatu berkas berekstensi .f di dalam folder fdat yang\n",
    "berisi daftar <i>user id</i> pada jaringan <i>friends-of-friends</i>. Untuk mengunduh data ini,\n",
    "diperlukan waktu sekitar 5-6 jam (bisa lebih lama, tergantung jumlah\n",
    "<i>follower</i>/<i>following</i> target). Hal ini disebabkan oleh <i>rate limit</i> dari Twitter API yang hanya memberikan akses data sebanyak 15 <i>requests</i> per 15 menit.\n",
    "\n",
    "5. Setelah selesai, eksekusi perintah berikut:\n",
    "```\n",
    "python twecoll edgelist <USERNAME_TWITTER_TARGET>\n",
    "```\n",
    "Perintah tersebut akan membentuk daftar <i>edge</i> dari daftar pertemanan yang sudah\n",
    "disimpan dalam folder fdat. Keluaran dari perintah ini adalah berupa berkas .gml\n",
    "dengan format <USERNAME_TWITTER_TARGET>.gml. Berikut adalah contoh isi berkas\n",
    ".gml dari <i>username</i> target ‚Äúnajwashihab‚Äù\n",
    "<img src='images/image4.png'></img>\n",
    "\n",
    "Untuk dapat melihat visualisasi antar <i>nodes</i>, berkas .gml pada langkah 5 dapat dibuka dengan menggunakan <i>graph processing tools</i> seperti Gephi atau dengan Python <i>library</i>, misalnya NetworkX dan igraph. Berikut adalah cara memvisualisikan berkas .gml pada Gephi:\n",
    "\n",
    "1. Saat membuka file .gml untuk pertama kali, akan muncul tampilan seperti ini\n",
    "<img src='images/image3.png'></img>\n",
    "\n",
    "2. Mengatur layout\n",
    "Graf yang ditunjukkan tidak beraturan dan susah untuk dianalisis. Di dalam Gephi \n",
    "terdapat beberapa <i>plugins</i> yang dapat digunakan untuk merepresentasikan kembali graf \n",
    "menjadi lebih terkelompok dan membuat <i>node</i> menjadi mudah untuk dilihat. <i>Layout</i> \n",
    "dapat diakses pada <i>box</i> pada pojok kiri bawah dari layar Gephi.\n",
    "<img src='images/image6.png'></img>\n",
    "\n",
    "3. Jika Anda klik dropdown `---Choose Layout`, maka akan muncul beberapa pilihan algoritma yang dapat dipilih seperti Force Atlas, Yifan Hu, dan lain-lain. Coba Anda pilih algoritma Force Atlas lalu <i>click</i> `Run`. Setelah beberapa saat lalu klik `Stop`, maka graf akan bergerak.\n",
    "<img src='images/image5.png'></img>\n",
    "\n",
    "4. Anda dapat menginstal beberapa plugins tambahan untuk visualisasi layout dengan\n",
    "masuk ke dalam <i>Tools</i> lalu <i>Plugins</i>.\n",
    "\n",
    "<i>Disclaimer</i>: Tidak ada tugas visualisasi <i>network</i> yang diujikan pada tutorial ini, namun hal tersebut akan berguna untuk <i>midterm project</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG-a7MuaaBWI"
   },
   "source": [
    "# Data Collection - Linkedin\n",
    "\n",
    "Salah satu cara untuk melakukan <i>scraping</i> data dari Linkedin adalah dengan menggunakan <i>library</i> Linkedin Scraper. Cara instalasi dan contoh penggunaan dapat teman-teman eksplor sendiri dari <a href=\"https://pypi.org/project/linkedin-scraper/\">halaman dokumentasi</a> <i>library</i> tersebut.\n",
    "\n",
    "<i>Library</i> Linkedin Scraper menggunakan <i>library</i> <i>Selenium</i> dalam melakukan <i>scraping</i> data. Jadi, sebelum memulai <i>scraping</i> Anda perlu mengunduh chromedriver sesuai dengan versi Chrome dan OS yang Anda gunakan. Chromedriver dapat diunduh pada halaman <a href=\"https://chromedriver.chromium.org/downloads\">berikut</a>.\n",
    "\n",
    "Berikut merupakan contoh kode yang digunakan untuk mengambil data dari suatu akun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrF62KFgaDNJ"
   },
   "outputs": [],
   "source": [
    "from linkedin_scraper import Person, actions\n",
    "from selenium import webdriver\n",
    "\n",
    "# masukkan path ke chrome driver\n",
    "driver = webdriver.Chrome(\n",
    "    \"\")\n",
    "\n",
    "email = \"\" # masukkan email akun Linkedin\n",
    "password = \"\" # masukkan password akun Linkedin\n",
    "actions.login(driver, email, password)\n",
    "person = Person(\"\", driver=driver, scrape=True) # masukkan url Linkedin Anda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXeXPZKDaGk8"
   },
   "outputs": [],
   "source": [
    "person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssK1CxpBUKxZ"
   },
   "source": [
    "# Data Collection - Web Scraping\n",
    "\n",
    "<i>Website</i> yang akan kita coba ambil datanya adalah alodokter.com, khususnya halaman <a href='https://www.alodokter.com/komunitas/diskusi/penyakit'>`Tanya Dokter`</a>. Kita akan mencoba mengambil data judul diskusi, penanya, dan pemberi balasan diskusi di halaman 2 (url: https://www.alodokter.com/komunitas/diskusi/penyakit/page/2) dengan menggunakan <i>library</i> Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T07:45:08.380063Z",
     "start_time": "2021-02-21T07:45:05.862336Z"
    },
    "id": "rK5CMZRxUKxa"
   },
   "outputs": [],
   "source": [
    "# import library\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "difpUFVxc5vm"
   },
   "source": [
    "<b>Code 10</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T08:12:44.815082Z",
     "start_time": "2021-02-21T08:12:39.893056Z"
    },
    "id": "gZt5iCMoUKxa",
    "outputId": "cd975b60-50fe-445d-b3f1-47242a1d9ddd"
   },
   "outputs": [],
   "source": [
    "# masukkan path ke chrome driver\n",
    "driver = webdriver.Chrome(\n",
    "    \"C:/Users/ASUS/chromedriver_win32/chromedriver.exe\")\n",
    "page = 2\n",
    "url = \"https://www.alodokter.com/komunitas/diskusi/penyakit/page/{}\".format(page)\n",
    "driver.get(url)\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located(\n",
    "    (By.XPATH, '//div[@id=\"topic-list\"]/*')))\n",
    "arr_cards = driver.find_elements_by_xpath('//div[@id=\"topic-list\"]/*')\n",
    "print('Banyak diskusi:', len(arr_cards))\n",
    "df_scrape = pd.DataFrame()\n",
    "for c in arr_cards:\n",
    "    judul = c.get_attribute(\"title\")\n",
    "    penanya = c.get_attribute(\"username\")\n",
    "    pemberi_balasan = c.get_attribute(\"pickup-name\")\n",
    "    slug = c.get_attribute(\"href\")\n",
    "    df_scrape = df_scrape.append({\n",
    "        \"slug\" : slug,\n",
    "        \"penanya\": penanya,\n",
    "        \"pemberi_balasan\": pemberi_balasan,\n",
    "        \"judul\": judul\n",
    "    }, ignore_index=True)\n",
    "df_scrape.to_csv(\"df_scrape_page_{}.csv\".format(page), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-21T08:24:05.809554Z",
     "start_time": "2021-02-21T08:24:05.788233Z"
    },
    "id": "zXxfgFFaUKxb",
    "outputId": "05adcff7-d18c-44fd-82ba-ec79552d94c3"
   },
   "outputs": [],
   "source": [
    "df_scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPrwEczJUKxb"
   },
   "source": [
    "Selain dengan menggunakan Selenium, kita juga bisa melakukan <i>scraping</i> data menggunakan <i>library</i> Requests. Bedanya dengan Selenium, <i>library</i> ini terbatas hanya dapat melakukan <i>scraping</i> pada <i>web</i> yang tidak begitu kompleks (bukan <i>web </i>berbasis <i>Javascript Core</i>). Berikut merupakan contoh penggunaan <i>Requests</i> dengan bantuan <i>library</i> BeautifulSoup untuk melakukan <i>scraping</i> halaman daftar obat pada <i>website</i> hellosehat.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T15:43:47.712515Z",
     "start_time": "2021-02-22T15:43:41.293438Z"
    },
    "id": "3Y3NooLxUKxb"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = requests.get('https://hellosehat.com/obatan-suplemen/obat/')\n",
    "soup = BeautifulSoup(page.content, 'lxml')\n",
    "arr_obat = soup.findAll('div', {'class': 'hb-hc-small-list-content'})\n",
    "arr_obat_str = []\n",
    "for o in arr_obat:\n",
    "    arr_obat_str.append(o.find('span').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T15:44:09.239553Z",
     "start_time": "2021-02-22T15:44:09.195810Z"
    },
    "id": "TUEOyKNYUKxb",
    "outputId": "aee7fad1-d0a9-4b12-f318-f0188b23ad4e"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(arr_obat_str).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nomor 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = requests.get('https://resepkoki.id/resep/resep-rendang-daging-sapi-padang/')\n",
    "soup = BeautifulSoup(page.content, 'lxml')\n",
    "arr_resep = soup.findAll('td')\n",
    "arr_resep_str = []\n",
    "for o in arr_resep:\n",
    "    arr_resep_str.append(o.find('span'))\n",
    "    print(o.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(arr_resep_str).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOXbq4uUURRi"
   },
   "source": [
    "# Tugas\n",
    "\n",
    "1. Ubah <b>Code 5</b> dan simpan hasil <i>crawling</i> menggunakan <i>code</i> tersebut dengan nama `hasil_tweet_c5.csv`. Hasil <i>crawling</i> yang disimpan berupa:\n",
    "    * Jika Anda laki-laki dan angka-angka NPM Anda ditotalkan menjadi bilangan genap: teks <i>tweet</i> dan <i>bio author</i>\n",
    "    * Jika Anda laki-laki dan angka-angka NPM Anda ditotalkan menjadi bilangan ganjil: teks <i>tweet</i> dan jumlah <i>follower author</i>\n",
    "    * Jika Anda perempuan dan angka-angka NPM Anda ditotalkan menjadi bilangan genap: teks <i>tweet</i> dan jumlah <i>favorite tweet</i> tersebut\n",
    "    * Jika Anda perempuan dan angka-angka NPM Anda ditotalkan menjadi bilangan ganjil: teks <i>tweet</i> dan jumlah <i>retweet tweet</i> tersebut\n",
    "\n",
    "2. Ubah <b>Code 9</b> dan simpan 10 <i>tweet</i> berbahasa Indonesia hasil <i>crawling</i> menggunakan <i>code</i> tersebut dengan nama `hasil_tweet_c9.csv`. Hasil <i>crawling</i> yang disimpan berupa:\n",
    "    * Jika angka kedua terakhir NPM Anda genap dan angkatan 2018: teks <i>tweet</i> dan warna <i>background</i> profil <i>author</i>\n",
    "    * Jika angka kedua terakhir NPM Anda genap dan bukan angkatan 2018: teks <i>tweet</i> dan warna teks profil <i>author</i>\n",
    "    * Jika angka kedua terakhir NPM Anda ganjil dan angkatan 2018: teks <i>tweet</i> dan informasi waktu <i>tweet</i> dibuat\n",
    "    * Jika angka kedua terakhir NPM Anda genap dan bukan angkatan 2018: teks <i>tweet</i> dan <i>id tweet</i>\n",
    "\n",
    "3. Dengan menggunakan Twitter Search, carilah 10 <i>tweet</i> populer dengan kata kunci\n",
    "sesuai pembagian berikut:\n",
    "<table>\n",
    "<tr><th>Angka terakhir pada NPM</th><th>Kata kunci</th></tr>\n",
    "<tr><td>0, 1, 2, 3</td><td>psbb OR ppkm</td></tr>\n",
    "<tr><td>4, 5, 6</td><td>‚Äòvirus corona‚Äô</td></tr>\n",
    "</table>\n",
    "Simpan <i>tweet</i> tersebut dengan nama `hasil_tweet_populer1.csv`\n",
    "\n",
    "4. Dengan menggunakan Twitter Search, carilah 10 <i>tweet</i> populer dengan kata kunci\n",
    "sesuai pembagian berikut:\n",
    "<table>\n",
    "<tr><th>Angka terakhir pada NPM</th><th>Kata kunci</th></tr>\n",
    "<tr><td>0, 1, 2, 3</td><td>psbb -ppkm</td></tr>\n",
    "<tr><td>4, 5, 6</td><td>‚Äòvirus OR corona‚Äô</td></tr>\n",
    "</table>\n",
    "Simpan <i>tweet</i> tersebut dengan nama `hasil_tweet_populer2.csv`\n",
    "\n",
    "5. Adakah perbedaan antara hasil no 3 dan no 4? Jika ada, apakah yang membedakan kedua hasil tersebut?\n",
    "\n",
    "6. Lakukan modifikasi pada <b>Code 10</b> agar <i>code</i> dapat mengekstrak informasi <i>slug post</i> sebuah diskusi. Contoh <i>output</i> yang diharapkan:\n",
    "<table>\n",
    "<tr><th>judul</th><th>pemberi_balasan</th><th>penanya</th><th>slug</th></tr>\n",
    "<tr><td>Mata kembali bengkak dan kemerahan di daerah bekas operasi kalazion</td><td>dr.Tabita Padmaya Setiawan</td><td>Yumi</td><td>/komunitas/topic/kelopak-mata-bengkak551b6e</td></tr>\n",
    "<tr><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
    "</table>\n",
    "\n",
    "7. Dengan menggunakan <i>library> requests</i>, <i>BeautifulSoup</i>, dan <i>json</i>, <i>crawling</i> bahan-bahan untuk membuat:\n",
    "    * <a href=\"https://resepkoki.id/resep/resep-rendang-daging-sapi-padang/\">Rendang daging sapi padang</a> jika jumlah huruf konsonan nama Anda genap\n",
    "    * <a href=\"https://resepkoki.id/resep/resep-bubur-ayam-quaker-oat/\">Bubur Ayam Quaker Oat</a> jika jumlah huruf konsonan nama Anda ganjil\n",
    "<br><br>Contoh <i>output</i> yang diharapkan dari resep <a href=\"https://resepkoki.id/resep/resep-es-alpukat/\">Es Alpukat</a>):\n",
    "```\n",
    "['Alpukat matang: 2 buah',\n",
    " 'Susu evaporasi (1 kaleng): 380 gram',\n",
    " 'Nata de coco, dan airnya: 250 gram',\n",
    " 'Susu kental manis putih: 60 ml',\n",
    " 'Selasih: 1 sdm',\n",
    " 'Pasta pandan: 5 tetes',\n",
    " 'Es batu: secukupnya']\n",
    "```\n",
    "\n",
    "8. [BONUS] Lakukan <i>scraping</i> profil Linkedin beberapa perusahaan e-Commerce terbesar di Indonesia sesuai dengan pembagian berikut:\n",
    "<table>\n",
    "<tr><th>Awalan huruf nama lengkap</th><th>URL Linkedin Perusahaan</th></tr>\n",
    "<tr><td>A-E</td><td>https://www.linkedin.com/company/pt--tokopedia/</td></tr>\n",
    "<tr><td>F-Q</td><td>https://www.linkedin.com/company/shopee/</td></tr>\n",
    "<tr><td>R-Z</td><td>https://www.linkedin.com/company/pt-bukalapak-com/</td></tr>\n",
    "</table>\n",
    "<i>Note:</i> Anda dapat menggunakan <i>code</i> <a href=\"https://github.com/joeyism/linkedin_scraper/blob/master/linkedin_scraper/company.py\">ini</a> sebagai referensi atau membuat <i>scraper</i> sendiri dari awal. Informasi yang perlu diambil adalah nama, <i>bio</i>, dan deskripsi singkat tentang perusahaan. Berikut merupakan contoh hasil <i>scraping</i> dari profil Linkedin Blibli.com:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"nama\": \"Blibli.com\",\n",
    "    \"bio\": \"Young generation who demonstrate R.E.S.P.E.C.T and dare to deliver impactful action for the nation!\"\n",
    "    \"deskripsi\": \"Blibli.com is an e-commerce platform built upon a big dream: to lead Indonesians to enter a new digital lifestyle and give only the best shopping experience for a better life. We are committed to conduct our business with customer-centric minded, striving to deliver the best features and services to satisfy our customers. We aim for an exciting challenge to become the number 1 e-commerce platform in Indonesia by providing the best customer-centric experience for both buyers and sellers. Blibli.com is created by Indonesians, in Indonesia, and for Indonesians; we‚Äôre here as an inspiration to channel and empower the Indonesian people to engage and socialize in commercial activities.\\n\\nWe understand this is a big challenge to be accomplished in one night. Yet, it‚Äôs a challenge we are ready to take. We realize a big dream like ours requires a big and stronger team. So, for our first step, we gather a group of talented and passionate people as our main power, and we spend a lot of quality time and effort to make our big dream come true.\\n\\nOur team consists of people who dare to dream and act big to reach their goals. We are the people who always go the extra miles only to provide the best service available passionately and proudly. We work together and become stronger by bringing in the best shopping experience for our customers.\\n\\nWe believe how we act within team will mirror the service that our customers receive. To achieve it, we believe in R.E.S.P.E.C.T. as the foundation of our daily action. By instilling R.E.S.P.E.C.T., we understand and build trust among the team and the customers. We actively fertilize our seed of big dream with their precious trust to Blibli.com. This then develops us to grow as a better company with the ever growing value, service, and product.\\n\\nThis is how we aim to deliver BIG CHOICES BIG DEALS for all. And in the end, just like a big and strong team we‚Äôre growing into, we could bring the best shopping experience for a better life!\"\n",
    "}\n",
    "```\n",
    "\n",
    "<i>Deadline</i> pengumpulan Tugas adalah Senin, 8 Maret 2021 pukul 08:00 di SCeLE. Berkas yang dikumpulkan adalah Tutorial 1 - Data Collection.ipynb dan seluruh berkas csv. Kumpulkan semua berkas dalam zip dengan format penamaan Tutorial1_Nama_NPM.zip."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial 1 - Data Collection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "259.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
